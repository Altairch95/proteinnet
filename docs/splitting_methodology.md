# Splitting Methodology
One of the routine but critical tasks in machine learning experiments is the splitting of data sets into three disjoint subsets, a training set to fit model parameters, a validation set to fit model hyperparamters (e.g. architectural choices), and a test set to assess model quality after all fitting is done and the model is ready for publication. One convenient way to think about the distinction between these subsets is the number of parameters being fitted and the number of times that the subset is evaluated. For training deep learning models, often millions of parameters (or more) are fit using the training set, through tens, hundreds, or even thousands of epochs of training. This means that the training set is evaluated thousands of times during the course of training. For fitting hyperparameters, typically a dozen to a hundred choices must be made, resulting in tens to hundreds of evaluations of the validation set. For the final model to be evaluated, there is nothing to be fit anymore, and the test is evaluated once. The number of evaluations made of a given subset is a heuristic of the degree of overfitting to each of these subsets. Having a robust division of data sets along these lines is thus critical for honest assessment of new methods and for the genuine advancement of machine learning methodology. Inadvertent "information leakage" from the validation set into the training set can lead to a false sense of progress, as the validation set ceases to be an accurate proxy of the distributional shifts that may be encountered in the test set. Worse, information leakage from the test set into the validation or (most egregiously) the training set can lead to overfitting on the test set itself, leading to an inaccurate and misleading assessment of model quality.

