# Splitting Methodology
One of routine but critical task in machine learning experiments is the splitting of data sets into three disjoint subsets, a training set to fit model parameters, a validation set to fit model hyperparamters (e.g. architectural choices), and a test set to assess model quality after all fitting is done and the model is ready for publication. One convenient way to think about the distinction between sets is the number of parameters being fitted and the number of evaluations that the subset gets. For training deep learning models, often millions of parameters or more are fit using the training set, through tens, hundreds, or even thousands of epochs of training. This means that the training set is evaluated thousands of times during the course of training. For fitting hyperparameters, typically a dozen to a hundred choices must be made, resulting in tens to hundreds of evaluations of the validation set. For the final model to be evaluated, there is nothing to be fit anymore, and the test is evaluated once. Having a robust division of data sets along these lines is critical for honest assessment of new methods and for the genuine advancement of machine learning methodology.
